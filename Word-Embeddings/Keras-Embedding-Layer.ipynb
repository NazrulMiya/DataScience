{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings - Using Embedding Layer\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a sequence of words. Like all other neural networks, deep-learning models donâ€™t take as input raw text:</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "they only work with numeric tensors. The process of transforming text into numeric vectors is called 'Vectorizing'.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "the different units into which you can break down text (words, characters,or n-grams) are called tokens, and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "There are two ways to associate a vectpr to tokens - One-Hot Encoding of Words and Word Embeddings.\n",
    "\n",
    "We will explore 'Word-Embeddings' in this article.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Word embeddings associates a dense vector with each token/word in a text corpus.These words vectors are low dimensional floating point vectors.These word vectors are learned from the data. There are two ways to obtain word embeddings.  \n",
    "    \n",
    "- Learn word embeddings with 'Embedding Layer'.\n",
    "\n",
    "- Load a pretrained model which has been trained on different text corpus. and get word vectors from the model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer  \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Learn word vectors or word embeddings by feding words into a neural network model and learn optimized weights by backpropagation through the model. These weights are then the word vectors for each tokens ina whole text corpus.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Keras has 'Embedding' layer which helps us to get the word vectors. It is a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Okay before we jump into embedding layer, lets first undtsrand what inputs this layer takes. We can not fed raw words directly into this layer. Tokens from text corpus needs to be preprocessed before we fed those into the layer. How do we prepare the text data for this embedding layer?</p>\n",
    "    \n",
    "<p style='text-align: justify;'>\n",
    "Well, The steps are first tokenize text data , clean data (cleaning special characters, numbers etc) , then convert tokens in each document in whole text corpus into sequence of integers and these sequences of integers correponds to token in each documents in a text corpus are then fed into emnbedding layer. Keras has powerful modules to achieve these steps.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Suppose we have below text corpus containing 3 text documents. and our task is to prepare this text corpus for our embedding layer which will return us the word vectors.</p>\n",
    "    \n",
    "    corpus = [\n",
    "              'I live in a country name India.',     --- Document 1\n",
    "              'India is a great country.',           --- Document 2\n",
    "              'I love my country very much.'         --- Document 3\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          'I live in a country name India.',     \n",
    "          'India is a great country.',           \n",
    "          'I love my country very much.'         \n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "As we know vectorization of words involves tokenization scheme to tokenize text data into list of tokens or words. Also we need to clean the text data meaning it must not contain any special characters, digits etc. And we may also wants to use certain number of words from our whole text . Keras has a module 'preprocessing.text' which has a class 'Tokenizer; which helps us to do these steps.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Corous -> Sequence of Word Indexes.... \n",
      " [[2, 5, 6, 3, 1, 7, 4], [4, 8, 3, 9, 1], [2, 10, 11, 1, 12, 13]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "vocab_size = 14 #(13 Unique Words plus 1)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size,\n",
    "                                                  lower = True,\n",
    "                                                  filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                                  split = ' ',\n",
    "                                                  char_level = False\n",
    "                                                 )\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "print('Text Corous -> Sequence of Word Indexes.... \\n', sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "The arguments are as follows,\n",
    "\n",
    "    num_words : (N-1) number of unique words in whole text corpus to be considered for word embeddings. Our corpus has 13 unique words and since we want word vectors for all unique words hence we have selected num_words= 14. which means (14-1)=13 most occuring unique words to be considered from our whole corpus.\n",
    "\n",
    "    lower : True , convert tokens into lower case.\n",
    "    \n",
    "    filters : Which characters to be cleaned from tokens. \n",
    "    \n",
    "    split : tokenization scheme. ' ' corresponds to tokenizing a document based on single space.\n",
    "    \n",
    "    char_level : True corresponds to tokenize into characters . False corresponds to tokenize text data into words.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    \n",
    "fit_on_texts() method of tokenizer takes a text corpus and applies above tokenization rules.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "finally texts_to_sequences() method convert tokens of each documents in a corpus into sequence of integers. Let see how it finds the integers.\n",
    "   \n",
    "Step 1: a OrderedDictinary is created where key is unique word and value is frequency of that word in whole corpus.\n",
    "\n",
    "Step 2: Sort the dictionary based on value.\n",
    "    \n",
    "Step 3: Assign an integer index sequentially starting from 1 to keys of the dictinary.\n",
    "    \n",
    "So in our example lets first create OrderedDictionary in the form (word, frequency)  \n",
    "> ('i', 2), ('live', 1), ('in', 1), ('a', 2), ('country', 3),  \n",
    "  ('name', 1), ('india', 2), ('is', 1), ('great', 1),  \n",
    "  ('love', 1), ('my', 1), ('very', 1), ('much', 1)  \n",
    "    \n",
    "Next sort the dictionary based on value and here it is,  \n",
    "> ('country', 3), ('i', 2), ('a', 2), ('india', 2),  \n",
    "  ('live', 1), ('in', 1), ('name', 1), ('is', 1),   \n",
    "  ('great', 1), ('love', 1), ('my', 1), ('very', 1), ('much', 1)\n",
    "    \n",
    "Next assign index to each words in the sorted dictionary ,  \n",
    "> {  \n",
    "       'country' : 1, 'i': 2 , 'a': 3, 'india' : 4,  \n",
    "       'live': 5, 'in' : 6, 'name' : 7, 'is' : 8,  \n",
    "       'great' : 9, 'love' : 10, 'my' : 11, 'very' : 12, 'much' : 13  \n",
    "  }\n",
    "    \n",
    "Now since num_words= 14, hence top (14-1)=13 words to be considered and those are nothing but all words across whole corpus.</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "First document in our corpus is ''I live in a country name India.' . I is there in our cosidered word list and it's index is 2. index of 'live' is 5 , index 6 for 'in' and son. finally the sequence for document 1 is [2, 5, 6, 3, 1, 7, 4].  \n",
    "    \n",
    "Similarwise sequence for tokens in document 2 'India is a great country.' is [4, 8, 3, 9, 1]  \n",
    "    \n",
    "and sequence for tokens in document 3 'I love my country very much.' is [2, 10, 11, 1, 12, 13]\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    So in above code we have prepocessed our text corpus into sequence of integers where integers corresponds to indices of words in documnets. Remember still we did not get word vectors. We are just prparing our text data to fed into neural network.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "    Next step in our data preparation, is to prepare a 2D array of shape (batch_size, sequence_length) beacuse this 2D array then to be passed as an input to Embedding layer.\n",
    "     So we have got list of sequences : [[2, 5, 6, 3, 1, 7, 4], [4, 8, 3, 9, 1], [2, 10, 11, 1, 12, 13]]. We will need a 2D array where batch_size=3 (because we have 3 documents in our corpus) and sequence_length=4 (because max length of sequenc is 7). the required 2D array should be as below,       \n",
    "    $$\\begin{bmatrix} 2 & 5 & 6 & 3 & 1 & 7 & 4 \\\\ 4 & 8 & 3 & 9 & 1 & 0 & 0 \\\\ 2 & 10 & 11 & 1 & 12 & 13 & 0 \\end{bmatrix}$$\n",
    "    \n",
    "Keras has beautiful module 'preprocessing.sequence' which helps to prepare the 2D array.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Array: \n",
      "\n",
      " [[ 2  5  6  3  1  7  4]\n",
      " [ 4  8  3  9  1  0  0]\n",
      " [ 2 10 11  1 12 13  0]]\n",
      "\n",
      " 2D Array Shape  (3, 7)\n"
     ]
    }
   ],
   "source": [
    "max_len = 7 # Max length in any of the documents\n",
    "padded_seq = tf.keras.preprocessing.sequence.pad_sequences(maxlen= 7,\n",
    "                                                           sequences=sequences,\n",
    "                                                           truncating='pre',\n",
    "                                                           padding='post')\n",
    "print('2D Array: \\n\\n', padded_seq)\n",
    "print('\\n 2D Array Shape ',padded_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Arguments : \n",
    "    \n",
    "    maxlen = max length of sequence. 7 in this example.  \n",
    "    \n",
    "    sequences = list of sequences of integers.  \n",
    "    \n",
    "    truncating = if a sequence length is greater than maxlen then how the sequence has to be truncated? 'pre' meaning truncates from begining, 'post' meaning truncates from ending.\n",
    "    \n",
    "    padding = if a sequence length is shorter than maxlen then how the sequence has to be padded? 'pre' meaning add 0s from begining, 'post' meaning add 0s towards ending.  \n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We have maxlen=7. 2nd sequence is of length 5 which is shorter than maxlen, hence a 0 needs to be padded and since padding='post' , 0 is padded at the end. 3rd sequence is of length 6 which is shorter than maxlen by 1, hence a 0 is padded at the end.\n",
    "    \n",
    "This is how we converted list of sequences of word indices into a 2D array of shape (3,7) where 3 corresponds to sample size (number of sequences) and 7 corresponds to max length of sequence.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "Finally with the help of keras preprocessing module our text corpus is ready to be feded into keras Embedding layer to generate word embeddings.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We will build a Sequential Model with a Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=8, input_length=max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Embedding layer takes below inputs,  \n",
    "\n",
    "    input_length : max length of sequence in input 2D array. In this case it is 7.\n",
    "        \n",
    "    input_dim : vocabulary size of our corpus which is 13. ( we have set 14 , remember always 1 extra). We wants word embeddings to be generated for all these 13 words  \n",
    "        \n",
    "    output_dim : what should be the dimension of generated word vector. It can be any dimansional depends on data corpus size. We have set 8.\n",
    "</p>\n",
    "\n",
    "Lets compile the model and then get the word embeddings using predict method of model passing padded sequences as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "word_embeddings = model.predict(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word_embeddings  (3, 7, 8)\n",
      " \n",
      "  [[[-0.03448143  0.02527387  0.04875959  0.03743121  0.00962205\n",
      "    0.0462733  -0.00197356 -0.03287669]\n",
      "  [-0.04859023  0.00589529  0.04115    -0.04330999  0.04140783\n",
      "   -0.02147198  0.02231913  0.04064867]\n",
      "  [-0.00984266  0.0279462  -0.02953575 -0.00323793  0.00286321\n",
      "    0.01004422 -0.03504807 -0.01536544]\n",
      "  [-0.01424579  0.02937368  0.0331848   0.00166159  0.03057465\n",
      "   -0.00994849  0.03966172  0.04327631]\n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612\n",
      "   -0.01934866 -0.01188357 -0.00877807]\n",
      "  [-0.02354031 -0.01780294 -0.03162633 -0.00448536 -0.04305381\n",
      "   -0.01177998  0.04270966  0.03985044]\n",
      "  [ 0.02116651 -0.03881351  0.01568277  0.02281796  0.04359093\n",
      "    0.01584264 -0.03330912 -0.0208941 ]]\n",
      "\n",
      " [[ 0.02116651 -0.03881351  0.01568277  0.02281796  0.04359093\n",
      "    0.01584264 -0.03330912 -0.0208941 ]\n",
      "  [-0.04705135 -0.02723608  0.02537857  0.02739319 -0.00563302\n",
      "   -0.00999812 -0.00948316 -0.0281214 ]\n",
      "  [-0.01424579  0.02937368  0.0331848   0.00166159  0.03057465\n",
      "   -0.00994849  0.03966172  0.04327631]\n",
      "  [-0.02410722  0.02107311 -0.0093237   0.0081773   0.02231826\n",
      "   -0.02058575  0.00239892 -0.01459477]\n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612\n",
      "   -0.01934866 -0.01188357 -0.00877807]\n",
      "  [-0.04812411  0.03417723 -0.01870621  0.00478382  0.02293907\n",
      "   -0.03638224  0.04484693 -0.03807062]\n",
      "  [-0.04812411  0.03417723 -0.01870621  0.00478382  0.02293907\n",
      "   -0.03638224  0.04484693 -0.03807062]]\n",
      "\n",
      " [[-0.03448143  0.02527387  0.04875959  0.03743121  0.00962205\n",
      "    0.0462733  -0.00197356 -0.03287669]\n",
      "  [-0.04945066  0.02533212 -0.04798534  0.01886434 -0.04956334\n",
      "   -0.03369478  0.04305983 -0.02115854]\n",
      "  [ 0.01056278 -0.02804018  0.02915039  0.02704266  0.02753152\n",
      "    0.02547929  0.0087726  -0.02509322]\n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612\n",
      "   -0.01934866 -0.01188357 -0.00877807]\n",
      "  [-0.03770475 -0.02065287 -0.04197443 -0.01736921 -0.0186795\n",
      "   -0.04828596 -0.03008685 -0.03755848]\n",
      "  [ 0.00330292  0.02497074 -0.01447     0.03691876 -0.04346465\n",
      "   -0.0047889   0.00386091 -0.01260058]\n",
      "  [-0.04812411  0.03417723 -0.01870621  0.00478382  0.02293907\n",
      "   -0.03638224  0.04484693 -0.03807062]]]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of word_embeddings ', word_embeddings.shape)\n",
    "print(' \\n ', word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "Look at the shape of word embeddings is (3, 7, 8). It is a 3D tensor of shape (batch_size, input_length, output_dim) . Embedding layer accepts 2D array of sequences and returns a 3D array which can be passed as an input to RNN or ConvNet.\n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "lets interpret the output of 3D array returned from embedding layer. The embedding layer has generated word vectors of 8 dimensions. From this 3D array we will get word vectors of each words in our corpus.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document:  I live in a country name India.\n",
      "\n",
      " ----- Word ----  I\n",
      " ----- Word Vector ----- \n",
      "  [-0.03448143  0.02527387  0.04875959  0.03743121  0.00962205  0.0462733\n",
      " -0.00197356 -0.03287669]\n",
      "\n",
      " ----- Word ----  live\n",
      " ----- Word Vector ----- \n",
      "  [-0.04859023  0.00589529  0.04115    -0.04330999  0.04140783 -0.02147198\n",
      "  0.02231913  0.04064867]\n",
      "\n",
      " ----- Word ----  in\n",
      " ----- Word Vector ----- \n",
      "  [-0.00984266  0.0279462  -0.02953575 -0.00323793  0.00286321  0.01004422\n",
      " -0.03504807 -0.01536544]\n",
      "\n",
      " ----- Word ----  a\n",
      " ----- Word Vector ----- \n",
      "  [-0.01424579  0.02937368  0.0331848   0.00166159  0.03057465 -0.00994849\n",
      "  0.03966172  0.04327631]\n",
      "\n",
      " ----- Word ----  country\n",
      " ----- Word Vector ----- \n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612 -0.01934866\n",
      " -0.01188357 -0.00877807]\n",
      "\n",
      " ----- Word ----  name\n",
      " ----- Word Vector ----- \n",
      "  [-0.02354031 -0.01780294 -0.03162633 -0.00448536 -0.04305381 -0.01177998\n",
      "  0.04270966  0.03985044]\n",
      "\n",
      " ----- Word ----  India.\n",
      " ----- Word Vector ----- \n",
      "  [ 0.02116651 -0.03881351  0.01568277  0.02281796  0.04359093  0.01584264\n",
      " -0.03330912 -0.0208941 ]\n",
      "\n",
      " Document:  India is a great country.\n",
      "\n",
      " ----- Word ----  India\n",
      " ----- Word Vector ----- \n",
      "  [ 0.02116651 -0.03881351  0.01568277  0.02281796  0.04359093  0.01584264\n",
      " -0.03330912 -0.0208941 ]\n",
      "\n",
      " ----- Word ----  is\n",
      " ----- Word Vector ----- \n",
      "  [-0.04705135 -0.02723608  0.02537857  0.02739319 -0.00563302 -0.00999812\n",
      " -0.00948316 -0.0281214 ]\n",
      "\n",
      " ----- Word ----  a\n",
      " ----- Word Vector ----- \n",
      "  [-0.01424579  0.02937368  0.0331848   0.00166159  0.03057465 -0.00994849\n",
      "  0.03966172  0.04327631]\n",
      "\n",
      " ----- Word ----  great\n",
      " ----- Word Vector ----- \n",
      "  [-0.02410722  0.02107311 -0.0093237   0.0081773   0.02231826 -0.02058575\n",
      "  0.00239892 -0.01459477]\n",
      "\n",
      " ----- Word ----  country.\n",
      " ----- Word Vector ----- \n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612 -0.01934866\n",
      " -0.01188357 -0.00877807]\n",
      "\n",
      " Document:  I love my country very much.\n",
      "\n",
      " ----- Word ----  I\n",
      " ----- Word Vector ----- \n",
      "  [-0.03448143  0.02527387  0.04875959  0.03743121  0.00962205  0.0462733\n",
      " -0.00197356 -0.03287669]\n",
      "\n",
      " ----- Word ----  love\n",
      " ----- Word Vector ----- \n",
      "  [-0.04945066  0.02533212 -0.04798534  0.01886434 -0.04956334 -0.03369478\n",
      "  0.04305983 -0.02115854]\n",
      "\n",
      " ----- Word ----  my\n",
      " ----- Word Vector ----- \n",
      "  [ 0.01056278 -0.02804018  0.02915039  0.02704266  0.02753152  0.02547929\n",
      "  0.0087726  -0.02509322]\n",
      "\n",
      " ----- Word ----  country\n",
      " ----- Word Vector ----- \n",
      "  [ 0.03490904 -0.045705    0.02629853 -0.0056543  -0.03333612 -0.01934866\n",
      " -0.01188357 -0.00877807]\n",
      "\n",
      " ----- Word ----  very\n",
      " ----- Word Vector ----- \n",
      "  [-0.03770475 -0.02065287 -0.04197443 -0.01736921 -0.0186795  -0.04828596\n",
      " -0.03008685 -0.03755848]\n",
      "\n",
      " ----- Word ----  much.\n",
      " ----- Word Vector ----- \n",
      "  [ 0.00330292  0.02497074 -0.01447     0.03691876 -0.04346465 -0.0047889\n",
      "  0.00386091 -0.01260058]\n"
     ]
    }
   ],
   "source": [
    "doc_indx = 0\n",
    "\n",
    "for documents in corpus:\n",
    "    print('\\n Document: ', documents)\n",
    "    word_indx = 0\n",
    "    for word in documents.split(' '):\n",
    "        print('\\n ----- Word ---- ', word)\n",
    "        \n",
    "        print(' ----- Word Vector ----- \\n ',word_embeddings[doc_indx, word_indx , :])\n",
    "        \n",
    "        word_indx += 1\n",
    "    doc_indx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must be thinking what are these numbers in word vectors. These are nothing but the weights associated to each item in each sequence and learned using backpropagation.\n",
    "\n",
    "Hope this article helps you to understand Keras Embedding Layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
