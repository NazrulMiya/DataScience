{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq model is machine learning approach used ofr loanguage processing. It is also known as 'Encoder-Decoder Architecture'. This model architecture is useful to solve usecases where input and output are both sequence of elements. For e.g. Translation of a language to another language( say French to English) here input is english sentence which is a sequence of words and also output is french sentence a sequence of symbols.There are many such usecases like image cationing, speech processing , conversational models and text summarization etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Seq2Seq model converts an input sequence to output sequence. The input and output sequences may differ in length. here is the architecture of Seq2Seq model. \n",
    "<img src='Seq2Seq-Architecture.png' style='align: right'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has two core component - 'Encoder' and 'Decoder'.  \n",
    "**Encoder -** The encoder component takes a sequence as an input and returns a fixed-dimensional vector containing the states of the input sequence.\n",
    "\n",
    "**Decoder -** The decoder component turns the vector into an output sequence. Decoder component is trained on both the output sequence aswell as the fixed representation from encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder component is built using an Embedding layer and a RNN (specialy LSTM/GRU to handle vanishing gradient problem adn long term dependency problem).  \n",
    "The Embdedding layer accepts a 2D array of input sequences , the shape of 2D array is (batch_size, sequence_length) where batch_size if number of input samples and sequence_length is maximum length of any sequence. The Embedding layer then returns a 3D array of shape(batch_size, input_length, output_dim) where input_length is maximum length of any sequence and output_dim is the fixed dimension of output vector.  \n",
    "\n",
    "This 3D array is then passed to RNN (LSTM/GRU). The RNN process an item in the sequence to its hidden layer and predicts the next item in the sequence.This predicted output is then passed as an input to next hidden layer along with next item in the input sequence and predicts another output ans so on. finally the output of last item in the sequence is converted into a vector of fixed size. \n",
    "\n",
    "Decoder component is implemented using a RNN model which takes the vector returned from encoder as an input and predicts the next item in the output sequence. While training this RNN , it is trained on items of output sequence given last state from encoder step which provides context to the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets learn the encoder decoder model using an example. In this example we will translate Bengali Sentences to English sentences. Data can be downloaded from http://www.manythings.org/anki/ \n",
    "\n",
    "So input sequence is 'Bengali Sentence' and Output sequence is 'English Sentence'.\n",
    "\n",
    "Lets first load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "ZT4r7lv5h18N",
    "outputId": "94775be8-686e-4d4b-d867-db50928531c5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair of Bengali and English senetences downloaded from the given site is in text file seperated by a tab. So we will first load the sentence pairs from the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2v-1n5eeh19T",
    "outputId": "42d89903-58a3-4184-e15c-031cf9db734d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যাও।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যান।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যা।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালাও!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালান!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1                                                  2\n",
       "0   Go.    যাও।  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1   Go.    যান।  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2   Go.     যা।  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3  Run!  পালাও!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "4  Run!  পালান!  CC-BY 2.0 (France) Attribution: tatoeba.org #9..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Bong-English-Sent-Pairs.txt', sep='\\t', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collumn '0' corresponds to English Sentences and collumn '1' corresponds to Bengali Sentence. We do not need collumn 2 hence we will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hY3o9ZM3h1-D"
   },
   "outputs": [],
   "source": [
    "data.drop(2, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "TSdtWe5Zh1-X",
    "outputId": "5cf9380c-e476-46c2-e558-53d1735b0daf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_Lang</th>\n",
       "      <th>Bengali_Lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যাও।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যান।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>যা।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালাও!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run!</td>\n",
       "      <td>পালান!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English_Lang Bengali_Lang\n",
       "0          Go.         যাও।\n",
       "1          Go.         যান।\n",
       "2          Go.          যা।\n",
       "3         Run!       পালাও!\n",
       "4         Run!       পালান!"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['English_Lang','Bengali_Lang']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "dr9NY85bh1_E",
    "outputId": "36a9e4d7-7422-434f-cf9a-fa7aa1d58f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4349 entries, 0 to 4348\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   English_Lang  4349 non-null   object\n",
      " 1   Bengali_Lang  4349 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 68.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3r9Rf_YIh1_3",
    "outputId": "78165722-0873-4e68-d205-1e271918629c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "English_Lang    0\n",
       "Bengali_Lang    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has 4349 sentence pairs and there are no missing records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4349, 2)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ = data.copy(deep=True)\n",
    "data_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out max length of both bengali sentence and english sentence becuase we need these to determine the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wrQkKhl5h2B0",
    "outputId": "a2b9c7a1-06d3-48bb-c2fb-a7a65e15d039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length of Bengali Phrase :  18\n"
     ]
    }
   ],
   "source": [
    "max_len_bong = data_.Bengali_Lang.apply(lambda x : len([w for w in x.split(' ')])).max()\n",
    "print('Max Length of Bengali Phrase : ', max_len_bong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oUmRwcrRh2CI",
    "outputId": "77faac9b-229c-42b5-be57-90412d31cc56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length of English Phrase :  19\n"
     ]
    }
   ],
   "source": [
    "max_len_eng = data_.English_Lang.apply(lambda x : len([w for w in x.split(' ')])).max()\n",
    "print('Max Length of English Phrase : ', max_len_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_.Bengali_Lang\n",
    "Y = data_.English_Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "bong_tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "bong_tokenizer.fit_on_texts(X)\n",
    "\n",
    "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)\n",
    "eng_tokenizer.fit_on_texts(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali Vocabulary Length  3321\n"
     ]
    }
   ],
   "source": [
    "bong_vocb_size = len(bong_tokenizer.word_index) + 1\n",
    "print('Bengali Vocabulary Length ', bong_vocb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Length  1876\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size =len(eng_tokenizer.word_index) + 1\n",
    "print('English Vocabulary Length ', eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not going to perform any kind of NLP preprocessing techniques such as stop word removals, stemming, lemmatization because this usecase is a language translation usecase. Each word in a sentence matters.\n",
    "Next lets split our data set into training and testing set. Remember X is Bengali Sentence and Y is English Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_S6G6iHJh2Ch",
    "outputId": "931def81-5330-47c6-dc28-2cda2fc8f521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3914,) (435,) (3914,) (435,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test , y_train, y_test = train_test_split(X, Y , test_size=0.1, random_state=42)\n",
    "print(x_train.shape , x_test.shape , y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now prepare the text data into sequence of integers where each integer corresponds to index of a word in that sequence. Because we can not directly pass text data to RNN. first we need to convert it into sequence . \n",
    "Keras Tokenization helps to converts sentences into sequence of integers. And once sequence is ready i will us 'pad_sequences' to padd 0 for shorter or bigger legth sequences. We need to convert both input and output sentences imto corresponding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7_9swLJah2C3",
    "outputId": "b345d6dc-9c69-499a-dd71-52f341d87597"
   },
   "outputs": [],
   "source": [
    "# Russian Sentences to Sequence\n",
    "x_train_seq = bong_tokenizer.texts_to_sequences(x_train)\n",
    "x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen= max_len_bong, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "\n",
    "# English Sentences to Sequence\n",
    "y_train_seq = eng_tokenizer.texts_to_sequences(y_train)\n",
    "y_train_seq = tf.keras.preprocessing.sequence.pad_sequences(y_train_seq, maxlen= max_len_eng, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used lower=True in the Tokenizer class , to allow lowering the words in a sentence. in pad_sequences i have used maxlen= max_len_bong which is 18 incase of bengali sentences and 19 incase of english sentences. maxlen tells what maximum length to consider for a sequence. if any sequence is shorter maxlen then 0 would be padded and if any sequence is bigger than maxlen then items in that sequence would be trimed off.   \n",
    "\n",
    "Vocabulary size is nothing but total unique words in whole text corpus. We can see consider whole english sentences gives 1876 unique words. and bengali sentences have overall 3321 unique words. This vocabulary size is required in Embedding layer because Embedding layer needs to understand for how many words/items embeddings have to be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SikFqN7lh2DY",
    "outputId": "5ae239f2-118d-44b0-f8fb-69c5b93fc503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3914, 18) (3914, 19)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_seq.shape, y_train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have both input and output sequences are ready. Input sequences are sequences of integers corresponds to each word in corresponding bengali sentence. and output sequences are sequences of integers coreesponds to words in english sentence. We are ready to build Encoder-Decoder model or Seq2Seq model.\n",
    "\n",
    "Encoder:\n",
    "We will first use Embedding layer to generate word embeddings for each word in whole input sentences. \n",
    "Next these word embeddings will be sent to LSTM model to save the state and return a fixed size vector of states from item in a sequence.\n",
    "\n",
    "Decoder:\n",
    "We will send these fixed sized vector of states from encoder into another LSTM model which will get trained on output sequences conditioned context passed from encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qOp0N-FLh2Dp"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim= bong_vocb_size, output_dim=512, input_length=max_len_bong))\n",
    "model.add(tf.keras.layers.LSTM(units=512))\n",
    "model.add(tf.keras.layers.RepeatVector(max_len_eng))\n",
    "model.add(tf.keras.layers.LSTM(units=512, return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(eng_vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the first step involves creating a keras sequential model.   \n",
    "\n",
    "The first layer in the Encoder component is a Embedding Layer , This will convert our words (referenced by integers in the data) into meaningful embedding vectors. This Embedding() layer takes the size of the vocabulary as its first argument, then the size of the resultant embedding vector that you want as the next argument. Finally, because this layer is the first layer in the network, we must specify the “length” of the input i.e. the number of steps/words in each sample. In this example shape of embedding   \n",
    "\n",
    "The next layer is the first of our two LSTM layers. To specify an LSTM layer, first we have to provide the number of nodes in the hidden layers within the LSTM cell,e.g. the number of cells in the forget gate layer, the tanh squashing input layer and so on. The next argument that is specified in the code above is the return_sequences=False argument. This argument ensures that LSTM cell returns the output of LSTM cell from last time step only. return_sequences=True ensures LSTM cell returns outputs from unrolled LSTM cell through all time stamps. As we have seen architecture diagram , only the output of last time step is given to decoder component , hence we have set return_sequences=False. <img src='Se2Seq-Return_Seq.png'>\n",
    "\n",
    "A LSTM has as many cells as timesteps. The second LSTM would be having 19 cells as we have maximum 19 timesteps for english phrases. Lets understand why do we need RepeatVector . In the Encoder-Decoder model we pass the output of the last timestep in the encoder LSTM layer, to the LSTM layer in Decoder component. Now The output from encoder component needs to be passed to each and evry timestep in Decoder LSOM layer. But we get oly a single vector in encoder's LSTM layer. How do we pass this single output vector to every timestep in next LSTM layer. Answer is RepeatVector. We repeat the same output vector as many times as the next LSTM layer has time stpes. In our usecase next LSTM layer (Decoder Comp) will have 19 timesteps because this LSTM will be trained on output sequence and output sequence is engalish sentence which has max 19 length.That is why we have set RepeatVector(max_len_eng=19) so that the output of LSTM layer in Encoder component is passed to every 19 timesteps in next LSTM layer. Below figure will clear it better. <img src='Repeat-Vector.png'>  \n",
    "\n",
    "Next layer is LSTM layer of Decoder component which will get trained on output sequence i.e eglish language sequence given the output of last timestep in the LSTM layer of Encoder. We are predicting the English words corresponds to input bengali words. so we need output from all timesteps in this LSTM layer. hence we have set return_sequences=True.\n",
    "\n",
    "Next layer is Dense layer or fully connected layer of length english vocabulary size. This layer is used with an activation of Softmax which will quash the output between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compile the model with optimizer='rmsprop' and loss='sparse_categorical_crossentropy'. Our usecase is to translate bengali sentences into english senetences. It is basically a multiclass classification problem where the model predicts the english word in the target english vocabulary given the russian sequence as input. We will use loss='sparse_categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the model with input sequences (x_train_seq) and output sequences (y_train_seq) . We will train our model with an epoch=200 and batch_size=512 and a validation_split=0.2.  \n",
    "\n",
    "Epoch is a hyperparemeter which specifies how many times entire dataset is passed forward and backward through the neural network only once. here we want entire dataset to be used 200 times for training the model.  \n",
    "\n",
    "batch_size = After how many training samples the gradient to be updated. Here batch_size=512 menas after every 512 samples in a epoch the model will update the weights through gradient descent.  \n",
    "\n",
    "validation_split=0.2 means 20% of training samples to be used for validation purpose and remaining 80% to be used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3914, 19, 1)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_seq = y_train_seq.reshape(y_train_seq.shape[0], y_train_seq.shape[1], 1)\n",
    "y_train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "22gNblWYh2Ev",
    "outputId": "e83466e1-21e7-40cd-be7d-15f948f92a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/200\n",
      "3131/3131 [==============================] - 74s 24ms/sample - loss: 3.6601 - accuracy: 0.6375 - val_loss: 1.8357 - val_accuracy: 0.7561\n",
      "Epoch 2/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 1.7726 - accuracy: 0.7613 - val_loss: 1.7244 - val_accuracy: 0.7638\n",
      "Epoch 3/200\n",
      "3131/3131 [==============================] - 62s 20ms/sample - loss: 1.6840 - accuracy: 0.7679 - val_loss: 1.6927 - val_accuracy: 0.7635\n",
      "Epoch 4/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 1.6143 - accuracy: 0.7700 - val_loss: 1.8136 - val_accuracy: 0.7627\n",
      "Epoch 5/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 1.5871 - accuracy: 0.7715 - val_loss: 1.6429 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 1.5700 - accuracy: 0.7719 - val_loss: 1.6518 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 1.5486 - accuracy: 0.7729 - val_loss: 1.6148 - val_accuracy: 0.7657\n",
      "Epoch 8/200\n",
      "3131/3131 [==============================] - 70s 22ms/sample - loss: 1.5372 - accuracy: 0.7726 - val_loss: 1.6897 - val_accuracy: 0.7670\n",
      "Epoch 9/200\n",
      "3131/3131 [==============================] - 123s 39ms/sample - loss: 1.5170 - accuracy: 0.7729 - val_loss: 1.6122 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "3131/3131 [==============================] - 131s 42ms/sample - loss: 1.5178 - accuracy: 0.7730 - val_loss: 1.6188 - val_accuracy: 0.7671\n",
      "Epoch 11/200\n",
      "3131/3131 [==============================] - 127s 41ms/sample - loss: 1.4972 - accuracy: 0.7731 - val_loss: 1.6019 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "3131/3131 [==============================] - 112s 36ms/sample - loss: 1.5020 - accuracy: 0.7730 - val_loss: 1.6390 - val_accuracy: 0.7670\n",
      "Epoch 13/200\n",
      "3131/3131 [==============================] - 113s 36ms/sample - loss: 1.4821 - accuracy: 0.7722 - val_loss: 1.6135 - val_accuracy: 0.7670\n",
      "Epoch 14/200\n",
      "3131/3131 [==============================] - 113s 36ms/sample - loss: 1.4797 - accuracy: 0.7730 - val_loss: 1.6101 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "3131/3131 [==============================] - 115s 37ms/sample - loss: 1.4613 - accuracy: 0.7728 - val_loss: 2.3351 - val_accuracy: 0.7432\n",
      "Epoch 16/200\n",
      "3131/3131 [==============================] - 115s 37ms/sample - loss: 1.6387 - accuracy: 0.7685 - val_loss: 1.6267 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "3131/3131 [==============================] - 114s 36ms/sample - loss: 1.4703 - accuracy: 0.7728 - val_loss: 1.6261 - val_accuracy: 0.7675\n",
      "Epoch 18/200\n",
      "3131/3131 [==============================] - 114s 36ms/sample - loss: 1.4678 - accuracy: 0.7729 - val_loss: 1.6264 - val_accuracy: 0.7657\n",
      "Epoch 19/200\n",
      "3131/3131 [==============================] - 93s 30ms/sample - loss: 1.4724 - accuracy: 0.7722 - val_loss: 1.6185 - val_accuracy: 0.7670\n",
      "Epoch 20/200\n",
      "3131/3131 [==============================] - 74s 24ms/sample - loss: 1.4537 - accuracy: 0.7729 - val_loss: 1.6579 - val_accuracy: 0.7676\n",
      "Epoch 21/200\n",
      "3131/3131 [==============================] - 75s 24ms/sample - loss: 1.4622 - accuracy: 0.7729 - val_loss: 1.6136 - val_accuracy: 0.7657\n",
      "Epoch 22/200\n",
      "3131/3131 [==============================] - 108s 35ms/sample - loss: 1.4442 - accuracy: 0.7723 - val_loss: 1.6197 - val_accuracy: 0.7670\n",
      "Epoch 23/200\n",
      "3131/3131 [==============================] - 108s 34ms/sample - loss: 1.4537 - accuracy: 0.7725 - val_loss: 1.6261 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "3131/3131 [==============================] - 110s 35ms/sample - loss: 1.4453 - accuracy: 0.7730 - val_loss: 1.6388 - val_accuracy: 0.7670\n",
      "Epoch 25/200\n",
      "3131/3131 [==============================] - 106s 34ms/sample - loss: 1.4411 - accuracy: 0.7735 - val_loss: 1.6216 - val_accuracy: 0.7676\n",
      "Epoch 26/200\n",
      "3131/3131 [==============================] - 112s 36ms/sample - loss: 1.4327 - accuracy: 0.7737 - val_loss: 1.6354 - val_accuracy: 0.7676\n",
      "Epoch 27/200\n",
      "3131/3131 [==============================] - 82s 26ms/sample - loss: 1.4502 - accuracy: 0.7737 - val_loss: 1.6009 - val_accuracy: 0.7700\n",
      "Epoch 28/200\n",
      "3131/3131 [==============================] - 69s 22ms/sample - loss: 1.4012 - accuracy: 0.7763 - val_loss: 1.5961 - val_accuracy: 0.7711\n",
      "Epoch 29/200\n",
      "3131/3131 [==============================] - 69s 22ms/sample - loss: 1.3724 - accuracy: 0.7774 - val_loss: 1.5689 - val_accuracy: 0.7726\n",
      "Epoch 30/200\n",
      "3131/3131 [==============================] - 70s 22ms/sample - loss: 1.3949 - accuracy: 0.7758 - val_loss: 1.5736 - val_accuracy: 0.7719\n",
      "Epoch 31/200\n",
      "3131/3131 [==============================] - 70s 22ms/sample - loss: 1.3404 - accuracy: 0.7782 - val_loss: 1.6660 - val_accuracy: 0.7664\n",
      "Epoch 32/200\n",
      "3131/3131 [==============================] - 69s 22ms/sample - loss: 1.3698 - accuracy: 0.7767 - val_loss: 1.5454 - val_accuracy: 0.7715\n",
      "Epoch 33/200\n",
      "3131/3131 [==============================] - 69s 22ms/sample - loss: 1.3481 - accuracy: 0.7767 - val_loss: 1.5827 - val_accuracy: 0.7727\n",
      "Epoch 34/200\n",
      "3131/3131 [==============================] - 77s 24ms/sample - loss: 1.3060 - accuracy: 0.7789 - val_loss: 1.6515 - val_accuracy: 0.7717\n",
      "Epoch 35/200\n",
      "3131/3131 [==============================] - 108s 35ms/sample - loss: 1.3326 - accuracy: 0.7773 - val_loss: 1.5416 - val_accuracy: 0.7729\n",
      "Epoch 36/200\n",
      "3131/3131 [==============================] - 117s 37ms/sample - loss: 1.3137 - accuracy: 0.7794 - val_loss: 1.5353 - val_accuracy: 0.7752\n",
      "Epoch 37/200\n",
      "3131/3131 [==============================] - 118s 38ms/sample - loss: 1.2778 - accuracy: 0.7812 - val_loss: 1.6412 - val_accuracy: 0.7719\n",
      "Epoch 38/200\n",
      "3131/3131 [==============================] - 105s 33ms/sample - loss: 1.2969 - accuracy: 0.7793 - val_loss: 1.5217 - val_accuracy: 0.7748\n",
      "Epoch 39/200\n",
      "3131/3131 [==============================] - 87s 28ms/sample - loss: 1.2775 - accuracy: 0.7810 - val_loss: 1.5562 - val_accuracy: 0.7739\n",
      "Epoch 40/200\n",
      "3131/3131 [==============================] - 92s 29ms/sample - loss: 1.2421 - accuracy: 0.7819 - val_loss: 1.5335 - val_accuracy: 0.7715\n",
      "Epoch 41/200\n",
      "3131/3131 [==============================] - 82s 26ms/sample - loss: 1.2553 - accuracy: 0.7814 - val_loss: 1.5080 - val_accuracy: 0.7756\n",
      "Epoch 42/200\n",
      "3131/3131 [==============================] - 77s 25ms/sample - loss: 1.2306 - accuracy: 0.7830 - val_loss: 1.5189 - val_accuracy: 0.7743\n",
      "Epoch 43/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 1.2109 - accuracy: 0.7839 - val_loss: 1.5191 - val_accuracy: 0.7744\n",
      "Epoch 44/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 1.2327 - accuracy: 0.7833 - val_loss: 1.5131 - val_accuracy: 0.7758\n",
      "Epoch 45/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 1.2046 - accuracy: 0.7846 - val_loss: 1.5136 - val_accuracy: 0.7762\n",
      "Epoch 46/200\n",
      "3131/3131 [==============================] - 67s 21ms/sample - loss: 1.1893 - accuracy: 0.7861 - val_loss: 1.5180 - val_accuracy: 0.7771\n",
      "Epoch 47/200\n",
      "3131/3131 [==============================] - 67s 21ms/sample - loss: 1.1812 - accuracy: 0.7858 - val_loss: 1.4839 - val_accuracy: 0.7778\n",
      "Epoch 48/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 1.1769 - accuracy: 0.7870 - val_loss: 1.5237 - val_accuracy: 0.7778\n",
      "Epoch 49/200\n",
      "3131/3131 [==============================] - 82s 26ms/sample - loss: 1.1585 - accuracy: 0.7866 - val_loss: 1.4851 - val_accuracy: 0.7766\n",
      "Epoch 50/200\n",
      "3131/3131 [==============================] - 76s 24ms/sample - loss: 1.1646 - accuracy: 0.7873 - val_loss: 1.5067 - val_accuracy: 0.7794\n",
      "Epoch 51/200\n",
      "3131/3131 [==============================] - 77s 25ms/sample - loss: 1.1466 - accuracy: 0.7877 - val_loss: 1.4810 - val_accuracy: 0.7776\n",
      "Epoch 52/200\n",
      "3131/3131 [==============================] - 72s 23ms/sample - loss: 1.1394 - accuracy: 0.7885 - val_loss: 1.5083 - val_accuracy: 0.7784\n",
      "Epoch 53/200\n",
      "3131/3131 [==============================] - 78s 25ms/sample - loss: 1.1211 - accuracy: 0.7892 - val_loss: 1.4846 - val_accuracy: 0.7779\n",
      "Epoch 54/200\n",
      "3131/3131 [==============================] - 79s 25ms/sample - loss: 1.1274 - accuracy: 0.7895 - val_loss: 1.5132 - val_accuracy: 0.7797\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131/3131 [==============================] - 70s 22ms/sample - loss: 1.1043 - accuracy: 0.7908 - val_loss: 1.4700 - val_accuracy: 0.7776\n",
      "Epoch 56/200\n",
      "3131/3131 [==============================] - 102s 33ms/sample - loss: 1.1086 - accuracy: 0.7905 - val_loss: 1.4906 - val_accuracy: 0.7807\n",
      "Epoch 57/200\n",
      "3131/3131 [==============================] - 79s 25ms/sample - loss: 1.0910 - accuracy: 0.7913 - val_loss: 1.4821 - val_accuracy: 0.7810\n",
      "Epoch 58/200\n",
      "3131/3131 [==============================] - 102s 33ms/sample - loss: 1.0687 - accuracy: 0.7932 - val_loss: 1.4973 - val_accuracy: 0.7759\n",
      "Epoch 59/200\n",
      "3131/3131 [==============================] - 87s 28ms/sample - loss: 1.0884 - accuracy: 0.7919 - val_loss: 1.4780 - val_accuracy: 0.7797\n",
      "Epoch 60/200\n",
      "3131/3131 [==============================] - 87s 28ms/sample - loss: 1.0634 - accuracy: 0.7927 - val_loss: 1.5438 - val_accuracy: 0.7809\n",
      "Epoch 61/200\n",
      "3131/3131 [==============================] - 71s 23ms/sample - loss: 1.0616 - accuracy: 0.7940 - val_loss: 1.4724 - val_accuracy: 0.7819\n",
      "Epoch 62/200\n",
      "3131/3131 [==============================] - 68s 22ms/sample - loss: 1.0512 - accuracy: 0.7948 - val_loss: 1.5351 - val_accuracy: 0.7781\n",
      "Epoch 63/200\n",
      "3131/3131 [==============================] - 67s 22ms/sample - loss: 1.0443 - accuracy: 0.7939 - val_loss: 1.4881 - val_accuracy: 0.7789\n",
      "Epoch 64/200\n",
      "3131/3131 [==============================] - 91s 29ms/sample - loss: 1.0257 - accuracy: 0.7959 - val_loss: 1.4936 - val_accuracy: 0.7815\n",
      "Epoch 65/200\n",
      "3131/3131 [==============================] - 74s 24ms/sample - loss: 1.0346 - accuracy: 0.7938 - val_loss: 1.4844 - val_accuracy: 0.7808\n",
      "Epoch 66/200\n",
      "3131/3131 [==============================] - 77s 25ms/sample - loss: 1.0210 - accuracy: 0.7952 - val_loss: 1.4991 - val_accuracy: 0.7760\n",
      "Epoch 67/200\n",
      "3131/3131 [==============================] - 74s 24ms/sample - loss: 1.0281 - accuracy: 0.7955 - val_loss: 1.4805 - val_accuracy: 0.7824\n",
      "Epoch 68/200\n",
      "3131/3131 [==============================] - 79s 25ms/sample - loss: 1.0094 - accuracy: 0.7964 - val_loss: 1.4742 - val_accuracy: 0.7802\n",
      "Epoch 69/200\n",
      "3131/3131 [==============================] - 111s 35ms/sample - loss: 0.9927 - accuracy: 0.7977 - val_loss: 1.4754 - val_accuracy: 0.7803\n",
      "Epoch 70/200\n",
      "3131/3131 [==============================] - 83s 26ms/sample - loss: 0.9941 - accuracy: 0.7977 - val_loss: 1.5014 - val_accuracy: 0.7820\n",
      "Epoch 71/200\n",
      "3131/3131 [==============================] - 89s 28ms/sample - loss: 1.0004 - accuracy: 0.7982 - val_loss: 1.4766 - val_accuracy: 0.7812\n",
      "Epoch 72/200\n",
      "3131/3131 [==============================] - 85s 27ms/sample - loss: 0.9817 - accuracy: 0.7986 - val_loss: 1.4824 - val_accuracy: 0.7813\n",
      "Epoch 73/200\n",
      "3131/3131 [==============================] - 80s 26ms/sample - loss: 0.9673 - accuracy: 0.8002 - val_loss: 1.4836 - val_accuracy: 0.7803\n",
      "Epoch 74/200\n",
      "3131/3131 [==============================] - 93s 30ms/sample - loss: 0.9804 - accuracy: 0.7982 - val_loss: 1.4768 - val_accuracy: 0.7817\n",
      "Epoch 75/200\n",
      "3131/3131 [==============================] - 83s 27ms/sample - loss: 0.9441 - accuracy: 0.8016 - val_loss: 1.5100 - val_accuracy: 0.7834\n",
      "Epoch 76/200\n",
      "3131/3131 [==============================] - 73s 23ms/sample - loss: 0.9764 - accuracy: 0.7998 - val_loss: 1.4923 - val_accuracy: 0.7796\n",
      "Epoch 77/200\n",
      "3131/3131 [==============================] - 68s 22ms/sample - loss: 0.9475 - accuracy: 0.8016 - val_loss: 1.5016 - val_accuracy: 0.7827\n",
      "Epoch 78/200\n",
      "3131/3131 [==============================] - 73s 23ms/sample - loss: 0.9454 - accuracy: 0.8009 - val_loss: 1.4874 - val_accuracy: 0.7801\n",
      "Epoch 79/200\n",
      "3131/3131 [==============================] - 81s 26ms/sample - loss: 0.9309 - accuracy: 0.8025 - val_loss: 1.4962 - val_accuracy: 0.7803\n",
      "Epoch 80/200\n",
      "3131/3131 [==============================] - 74s 24ms/sample - loss: 0.9222 - accuracy: 0.8041 - val_loss: 1.5219 - val_accuracy: 0.7850\n",
      "Epoch 81/200\n",
      "3131/3131 [==============================] - 67s 21ms/sample - loss: 0.9327 - accuracy: 0.8034 - val_loss: 1.4963 - val_accuracy: 0.7821\n",
      "Epoch 82/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.9153 - accuracy: 0.8040 - val_loss: 1.5048 - val_accuracy: 0.7838\n",
      "Epoch 83/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.9058 - accuracy: 0.8065 - val_loss: 1.4908 - val_accuracy: 0.7797\n",
      "Epoch 84/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.9157 - accuracy: 0.8045 - val_loss: 1.5000 - val_accuracy: 0.7841\n",
      "Epoch 85/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.9028 - accuracy: 0.8050 - val_loss: 1.4921 - val_accuracy: 0.7840\n",
      "Epoch 86/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.9007 - accuracy: 0.8055 - val_loss: 1.4798 - val_accuracy: 0.7832\n",
      "Epoch 87/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.8958 - accuracy: 0.8069 - val_loss: 1.5200 - val_accuracy: 0.7846\n",
      "Epoch 88/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.8778 - accuracy: 0.8087 - val_loss: 1.4769 - val_accuracy: 0.7850\n",
      "Epoch 89/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.8742 - accuracy: 0.8081 - val_loss: 1.4765 - val_accuracy: 0.7845\n",
      "Epoch 90/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.8683 - accuracy: 0.8100 - val_loss: 1.5211 - val_accuracy: 0.7866\n",
      "Epoch 91/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.8593 - accuracy: 0.8116 - val_loss: 1.4963 - val_accuracy: 0.7827\n",
      "Epoch 92/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.8723 - accuracy: 0.8086 - val_loss: 1.4557 - val_accuracy: 0.7868\n",
      "Epoch 93/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.8417 - accuracy: 0.8136 - val_loss: 1.4786 - val_accuracy: 0.7865\n",
      "Epoch 94/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.8322 - accuracy: 0.8144 - val_loss: 1.4718 - val_accuracy: 0.7866\n",
      "Epoch 95/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.8346 - accuracy: 0.8148 - val_loss: 1.4828 - val_accuracy: 0.7867\n",
      "Epoch 96/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.8286 - accuracy: 0.8148 - val_loss: 1.4792 - val_accuracy: 0.7880\n",
      "Epoch 97/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.8134 - accuracy: 0.8171 - val_loss: 1.4660 - val_accuracy: 0.7850\n",
      "Epoch 98/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.8097 - accuracy: 0.8184 - val_loss: 1.5126 - val_accuracy: 0.7862\n",
      "Epoch 99/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.8216 - accuracy: 0.8161 - val_loss: 1.4562 - val_accuracy: 0.7899\n",
      "Epoch 100/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7803 - accuracy: 0.8221 - val_loss: 1.4796 - val_accuracy: 0.7905\n",
      "Epoch 101/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.8083 - accuracy: 0.8186 - val_loss: 1.4660 - val_accuracy: 0.7893\n",
      "Epoch 102/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.7831 - accuracy: 0.8213 - val_loss: 1.4478 - val_accuracy: 0.7906\n",
      "Epoch 103/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7543 - accuracy: 0.8270 - val_loss: 1.4804 - val_accuracy: 0.7908\n",
      "Epoch 104/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.7738 - accuracy: 0.8239 - val_loss: 1.4523 - val_accuracy: 0.7908\n",
      "Epoch 105/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7677 - accuracy: 0.8241 - val_loss: 1.4631 - val_accuracy: 0.7922\n",
      "Epoch 106/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.7383 - accuracy: 0.8284 - val_loss: 1.4445 - val_accuracy: 0.7914\n",
      "Epoch 107/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7450 - accuracy: 0.8277 - val_loss: 1.4479 - val_accuracy: 0.7873\n",
      "Epoch 108/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7392 - accuracy: 0.8301 - val_loss: 1.4440 - val_accuracy: 0.7957\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.7141 - accuracy: 0.8332 - val_loss: 1.4574 - val_accuracy: 0.7959\n",
      "Epoch 110/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.7171 - accuracy: 0.8330 - val_loss: 1.4498 - val_accuracy: 0.7935\n",
      "Epoch 111/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.7078 - accuracy: 0.8345 - val_loss: 1.4297 - val_accuracy: 0.7942\n",
      "Epoch 112/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.7096 - accuracy: 0.8342 - val_loss: 1.4717 - val_accuracy: 0.7928\n",
      "Epoch 113/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6844 - accuracy: 0.8383 - val_loss: 1.4191 - val_accuracy: 0.7960\n",
      "Epoch 114/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6779 - accuracy: 0.8402 - val_loss: 1.4260 - val_accuracy: 0.7956\n",
      "Epoch 115/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6802 - accuracy: 0.8376 - val_loss: 1.4561 - val_accuracy: 0.7982\n",
      "Epoch 116/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6845 - accuracy: 0.8387 - val_loss: 1.4265 - val_accuracy: 0.7984\n",
      "Epoch 117/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6522 - accuracy: 0.8440 - val_loss: 1.4565 - val_accuracy: 0.7992\n",
      "Epoch 118/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.6492 - accuracy: 0.8445 - val_loss: 1.4443 - val_accuracy: 0.7969\n",
      "Epoch 119/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6479 - accuracy: 0.8450 - val_loss: 1.4188 - val_accuracy: 0.7967\n",
      "Epoch 120/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6457 - accuracy: 0.8453 - val_loss: 1.4349 - val_accuracy: 0.8004\n",
      "Epoch 121/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.6061 - accuracy: 0.8525 - val_loss: 1.4126 - val_accuracy: 0.7996\n",
      "Epoch 122/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6310 - accuracy: 0.8490 - val_loss: 1.4350 - val_accuracy: 0.8008\n",
      "Epoch 123/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.6192 - accuracy: 0.8489 - val_loss: 1.4300 - val_accuracy: 0.7974\n",
      "Epoch 124/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.6019 - accuracy: 0.8524 - val_loss: 1.4183 - val_accuracy: 0.8008\n",
      "Epoch 125/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.5892 - accuracy: 0.8559 - val_loss: 1.4155 - val_accuracy: 0.7983\n",
      "Epoch 126/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.5932 - accuracy: 0.8548 - val_loss: 1.4296 - val_accuracy: 0.8017\n",
      "Epoch 127/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.5785 - accuracy: 0.8581 - val_loss: 1.4230 - val_accuracy: 0.7993\n",
      "Epoch 128/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.5699 - accuracy: 0.8587 - val_loss: 1.4377 - val_accuracy: 0.8032\n",
      "Epoch 129/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.5768 - accuracy: 0.8571 - val_loss: 1.4036 - val_accuracy: 0.8019\n",
      "Epoch 130/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.5487 - accuracy: 0.8628 - val_loss: 1.4143 - val_accuracy: 0.8039\n",
      "Epoch 131/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.5494 - accuracy: 0.8635 - val_loss: 1.4063 - val_accuracy: 0.8041\n",
      "Epoch 132/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.5324 - accuracy: 0.8662 - val_loss: 1.4043 - val_accuracy: 0.8057\n",
      "Epoch 133/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.5609 - accuracy: 0.8604 - val_loss: 1.4034 - val_accuracy: 0.8022\n",
      "Epoch 134/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.5266 - accuracy: 0.8670 - val_loss: 1.4290 - val_accuracy: 0.8053\n",
      "Epoch 135/200\n",
      "3131/3131 [==============================] - 69s 22ms/sample - loss: 0.5197 - accuracy: 0.8696 - val_loss: 1.4094 - val_accuracy: 0.8007\n",
      "Epoch 136/200\n",
      "3131/3131 [==============================] - 67s 21ms/sample - loss: 0.5175 - accuracy: 0.8691 - val_loss: 1.4359 - val_accuracy: 0.8056\n",
      "Epoch 137/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.5044 - accuracy: 0.8714 - val_loss: 1.4123 - val_accuracy: 0.8049\n",
      "Epoch 138/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.4942 - accuracy: 0.8739 - val_loss: 1.4218 - val_accuracy: 0.8067\n",
      "Epoch 139/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.4986 - accuracy: 0.8746 - val_loss: 1.4332 - val_accuracy: 0.7981\n",
      "Epoch 140/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4803 - accuracy: 0.8774 - val_loss: 1.4120 - val_accuracy: 0.8072\n",
      "Epoch 141/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4731 - accuracy: 0.8782 - val_loss: 1.4124 - val_accuracy: 0.8069\n",
      "Epoch 142/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.4732 - accuracy: 0.8780 - val_loss: 1.4010 - val_accuracy: 0.8068\n",
      "Epoch 143/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4659 - accuracy: 0.8804 - val_loss: 1.4385 - val_accuracy: 0.8108\n",
      "Epoch 144/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.4542 - accuracy: 0.8827 - val_loss: 1.4173 - val_accuracy: 0.8060\n",
      "Epoch 145/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.4436 - accuracy: 0.8853 - val_loss: 1.4187 - val_accuracy: 0.8074\n",
      "Epoch 146/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4476 - accuracy: 0.8849 - val_loss: 1.4089 - val_accuracy: 0.8057\n",
      "Epoch 147/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.4408 - accuracy: 0.8874 - val_loss: 1.4171 - val_accuracy: 0.8124\n",
      "Epoch 148/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.4279 - accuracy: 0.8889 - val_loss: 1.4073 - val_accuracy: 0.8096\n",
      "Epoch 149/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4169 - accuracy: 0.8923 - val_loss: 1.4143 - val_accuracy: 0.8035\n",
      "Epoch 150/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.4257 - accuracy: 0.8896 - val_loss: 1.4373 - val_accuracy: 0.8097\n",
      "Epoch 151/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4075 - accuracy: 0.8958 - val_loss: 1.4184 - val_accuracy: 0.8041\n",
      "Epoch 152/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4053 - accuracy: 0.8971 - val_loss: 1.4392 - val_accuracy: 0.8082\n",
      "Epoch 153/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.4023 - accuracy: 0.8960 - val_loss: 1.3940 - val_accuracy: 0.8136\n",
      "Epoch 154/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.3900 - accuracy: 0.8984 - val_loss: 1.4072 - val_accuracy: 0.8120\n",
      "Epoch 155/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.3964 - accuracy: 0.8968 - val_loss: 1.4077 - val_accuracy: 0.8142\n",
      "Epoch 156/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.3701 - accuracy: 0.9051 - val_loss: 1.4338 - val_accuracy: 0.8094\n",
      "Epoch 157/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.3830 - accuracy: 0.9007 - val_loss: 1.4113 - val_accuracy: 0.8080\n",
      "Epoch 158/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.3608 - accuracy: 0.9073 - val_loss: 1.4158 - val_accuracy: 0.8160\n",
      "Epoch 159/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.3557 - accuracy: 0.9083 - val_loss: 1.3891 - val_accuracy: 0.8140\n",
      "Epoch 160/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.3551 - accuracy: 0.9073 - val_loss: 1.4112 - val_accuracy: 0.8156\n",
      "Epoch 161/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.3394 - accuracy: 0.9126 - val_loss: 1.3983 - val_accuracy: 0.8131\n",
      "Epoch 162/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.3494 - accuracy: 0.9095 - val_loss: 1.4289 - val_accuracy: 0.8140\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.3429 - accuracy: 0.9108 - val_loss: 1.4262 - val_accuracy: 0.8110\n",
      "Epoch 164/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.3184 - accuracy: 0.9192 - val_loss: 1.4264 - val_accuracy: 0.8173\n",
      "Epoch 165/200\n",
      "3131/3131 [==============================] - 64s 21ms/sample - loss: 0.3322 - accuracy: 0.9142 - val_loss: 1.4032 - val_accuracy: 0.8133\n",
      "Epoch 166/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.3245 - accuracy: 0.9165 - val_loss: 1.4032 - val_accuracy: 0.8122\n",
      "Epoch 167/200\n",
      "3131/3131 [==============================] - 67s 21ms/sample - loss: 0.3123 - accuracy: 0.9202 - val_loss: 1.4111 - val_accuracy: 0.8197\n",
      "Epoch 168/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2941 - accuracy: 0.9249 - val_loss: 1.4194 - val_accuracy: 0.8166\n",
      "Epoch 169/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.3131 - accuracy: 0.9194 - val_loss: 1.4200 - val_accuracy: 0.8133\n",
      "Epoch 170/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.2967 - accuracy: 0.9237 - val_loss: 1.4185 - val_accuracy: 0.8193\n",
      "Epoch 171/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2778 - accuracy: 0.9307 - val_loss: 1.4035 - val_accuracy: 0.8168\n",
      "Epoch 172/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.2814 - accuracy: 0.9291 - val_loss: 1.4051 - val_accuracy: 0.8151\n",
      "Epoch 173/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.2993 - accuracy: 0.9218 - val_loss: 1.4316 - val_accuracy: 0.8183\n",
      "Epoch 174/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2758 - accuracy: 0.9300 - val_loss: 1.3981 - val_accuracy: 0.8222\n",
      "Epoch 175/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2589 - accuracy: 0.9351 - val_loss: 1.4264 - val_accuracy: 0.8172\n",
      "Epoch 176/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2549 - accuracy: 0.9375 - val_loss: 1.4587 - val_accuracy: 0.8188\n",
      "Epoch 177/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2942 - accuracy: 0.9238 - val_loss: 1.4157 - val_accuracy: 0.8201\n",
      "Epoch 178/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2387 - accuracy: 0.9413 - val_loss: 1.4171 - val_accuracy: 0.8200\n",
      "Epoch 179/200\n",
      "3131/3131 [==============================] - 68s 22ms/sample - loss: 0.2548 - accuracy: 0.9342 - val_loss: 1.4023 - val_accuracy: 0.8235\n",
      "Epoch 180/200\n",
      "3131/3131 [==============================] - 66s 21ms/sample - loss: 0.2392 - accuracy: 0.9404 - val_loss: 1.4423 - val_accuracy: 0.8222\n",
      "Epoch 181/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.2407 - accuracy: 0.9404 - val_loss: 1.4250 - val_accuracy: 0.8175\n",
      "Epoch 182/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2344 - accuracy: 0.9425 - val_loss: 1.4081 - val_accuracy: 0.8240\n",
      "Epoch 183/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.2271 - accuracy: 0.9441 - val_loss: 1.4257 - val_accuracy: 0.8182\n",
      "Epoch 184/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2296 - accuracy: 0.9424 - val_loss: 1.4039 - val_accuracy: 0.8248\n",
      "Epoch 185/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2249 - accuracy: 0.9432 - val_loss: 1.4069 - val_accuracy: 0.8252\n",
      "Epoch 186/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.2016 - accuracy: 0.9513 - val_loss: 1.4204 - val_accuracy: 0.8226\n",
      "Epoch 187/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.2030 - accuracy: 0.9499 - val_loss: 1.4190 - val_accuracy: 0.8237\n",
      "Epoch 188/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2098 - accuracy: 0.9484 - val_loss: 1.4726 - val_accuracy: 0.8143\n",
      "Epoch 189/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.2001 - accuracy: 0.9512 - val_loss: 1.4697 - val_accuracy: 0.8258\n",
      "Epoch 190/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.2249 - accuracy: 0.9421 - val_loss: 1.4167 - val_accuracy: 0.8249\n",
      "Epoch 191/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.1740 - accuracy: 0.9599 - val_loss: 1.4132 - val_accuracy: 0.8254\n",
      "Epoch 192/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.1725 - accuracy: 0.9599 - val_loss: 1.4262 - val_accuracy: 0.8277\n",
      "Epoch 193/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.1823 - accuracy: 0.9563 - val_loss: 1.4770 - val_accuracy: 0.8147\n",
      "Epoch 194/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.1891 - accuracy: 0.9547 - val_loss: 1.4235 - val_accuracy: 0.8264\n",
      "Epoch 195/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.1659 - accuracy: 0.9615 - val_loss: 1.4477 - val_accuracy: 0.8246\n",
      "Epoch 196/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.1754 - accuracy: 0.9574 - val_loss: 1.4679 - val_accuracy: 0.8239\n",
      "Epoch 197/200\n",
      "3131/3131 [==============================] - 65s 21ms/sample - loss: 0.1868 - accuracy: 0.9537 - val_loss: 1.4308 - val_accuracy: 0.8289\n",
      "Epoch 198/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.1394 - accuracy: 0.9702 - val_loss: 1.4261 - val_accuracy: 0.8271\n",
      "Epoch 199/200\n",
      "3131/3131 [==============================] - 63s 20ms/sample - loss: 0.1595 - accuracy: 0.9624 - val_loss: 1.4490 - val_accuracy: 0.8229\n",
      "Epoch 200/200\n",
      "3131/3131 [==============================] - 64s 20ms/sample - loss: 0.1720 - accuracy: 0.9574 - val_loss: 1.4424 - val_accuracy: 0.8198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d60b77bda0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_seq, y_train_seq, epochs=200, batch_size=512, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Seq2Seq model has achieved an accuracy of 95%. We can increase this by training model on more traning samples.  \n",
    "\n",
    "Now we will use this trained encoder-decoder model to translate a given bengali sentence in the tseting data set into english sentence.  First we will convert test bengali sentences into sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_6Y5pwYD2Li4"
   },
   "outputs": [],
   "source": [
    "x_test_seq = bong_tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen= max_len_rus,padding='post',truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  95,  100,   30, ...,    0,    0,    0],\n",
       "       [  10,   31,   41, ...,    0,    0,    0],\n",
       "       [  10,  970,   81, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,  367, 2319, ...,    0,    0,    0],\n",
       "       [  42,   37,  171, ...,    0,    0,    0],\n",
       "       [   4, 1723,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict output sequence passing x_test_seq into predict_classes method of model. model also has predict method but it would give the probability value of each class. where as predict_classes gives class with highest prob value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B63nWQXP1paF"
   },
   "outputs": [],
   "source": [
    "preds = model.predict_classes(x_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,    5,   50, ...,    0,    0,    0],\n",
       "       [  10,    3,   54, ...,    0,    0,    0],\n",
       "       [  90,   16,  422, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   19,    7, ...,    0,    0,    0],\n",
       "       [   2,  181,  155, ...,    0,    0,    0],\n",
       "       [  20,  847, 1176, ...,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have predicted output sequence. Lets get corresponding sentence for each predicted sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(eng_tokenizer.word_index.keys())\n",
    "predicted_value = [' '.join([words[y-1] for y in y_pred if y>0]) for y_pred in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Input': x_test,\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_value\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>আমি কখনই বলবো না।</td>\n",
       "      <td>I'll never tell.</td>\n",
       "      <td>i have learning french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>আমি আপনার সহায়তা করবো।</td>\n",
       "      <td>I'll assist you.</td>\n",
       "      <td>i'm your friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>নিজেকে বিভ্রান্ত কোরো না।</td>\n",
       "      <td>Don't delude yourself.</td>\n",
       "      <td>don't delude yourself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>সে কি শিক্ষক?</td>\n",
       "      <td>Is he a teacher?</td>\n",
       "      <td>is he a teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>আপনারা কি ডাক্তার?</td>\n",
       "      <td>Are you doctors?</td>\n",
       "      <td>are you a doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>সবাই এটা দেখেছিলো।</td>\n",
       "      <td>Everyone saw it.</td>\n",
       "      <td>everybody saw it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>আমরা ডাক্তার।</td>\n",
       "      <td>We are doctors.</td>\n",
       "      <td>we're are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>তুই কি টমের উপর নজর রাখতে পারবি?</td>\n",
       "      <td>Can you keep an eye on Tom?</td>\n",
       "      <td>can you keep an eye on tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>আমি এই ব্যাপারে খুব একটা ভালো নই।</td>\n",
       "      <td>I'm not very good at this.</td>\n",
       "      <td>i finally found my my well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>টম আহত।</td>\n",
       "      <td>Tom is hurt.</td>\n",
       "      <td>tom's injured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>চিৎকার করা বন্ধ করুন।</td>\n",
       "      <td>Stop screaming.</td>\n",
       "      <td>stop shouting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>মন দিয়ে শুনুন।</td>\n",
       "      <td>Listen carefully.</td>\n",
       "      <td>listen carefully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>আপনি কি অনুগ্রহ করে আরো ধীরে ধীরে কথা বলতে পার...</td>\n",
       "      <td>Could you speak more slowly, please?</td>\n",
       "      <td>could you please speak little little please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>নিশ্চই!</td>\n",
       "      <td>Definitely!</td>\n",
       "      <td>of course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>টম আহত।</td>\n",
       "      <td>Tom is wounded.</td>\n",
       "      <td>tom's injured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>এলাম!</td>\n",
       "      <td>Goodbye!</td>\n",
       "      <td>call the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>কোন টুপিটা তোমার?</td>\n",
       "      <td>Which cap is yours?</td>\n",
       "      <td>which cap is yours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>এখানেই থামুন।</td>\n",
       "      <td>Stop right here.</td>\n",
       "      <td>you go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>টমের পিছু নিন।</td>\n",
       "      <td>Follow Tom.</td>\n",
       "      <td>follow tom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>আমি ফিরে এসেছি।</td>\n",
       "      <td>I have returned.</td>\n",
       "      <td>i'm right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Input  \\\n",
       "1210                                  আমি কখনই বলবো না।   \n",
       "1200                            আমি আপনার সহায়তা করবো।   \n",
       "2247                          নিজেকে বিভ্রান্ত কোরো না।   \n",
       "1257                                      সে কি শিক্ষক?   \n",
       "1075                                 আপনারা কি ডাক্তার?   \n",
       "1116                                 সবাই এটা দেখেছিলো।   \n",
       "1052                                      আমরা ডাক্তার।   \n",
       "3004                   তুই কি টমের উপর নজর রাখতে পারবি?   \n",
       "2924                  আমি এই ব্যাপারে খুব একটা ভালো নই।   \n",
       "621                                             টম আহত।   \n",
       "1017                              চিৎকার করা বন্ধ করুন।   \n",
       "1502                                     মন দিয়ে শুনুন।   \n",
       "3912  আপনি কি অনুগ্রহ করে আরো ধীরে ধীরে কথা বলতে পার...   \n",
       "371                                             নিশ্চই!   \n",
       "1038                                            টম আহত।   \n",
       "70                                                এলাম!   \n",
       "1902                                  কোন টুপিটা তোমার?   \n",
       "1313                                      এখানেই থামুন।   \n",
       "387                                      টমের পিছু নিন।   \n",
       "1163                                    আমি ফিরে এসেছি।   \n",
       "\n",
       "                                    Actual  \\\n",
       "1210                      I'll never tell.   \n",
       "1200                      I'll assist you.   \n",
       "2247                Don't delude yourself.   \n",
       "1257                      Is he a teacher?   \n",
       "1075                      Are you doctors?   \n",
       "1116                      Everyone saw it.   \n",
       "1052                       We are doctors.   \n",
       "3004           Can you keep an eye on Tom?   \n",
       "2924            I'm not very good at this.   \n",
       "621                           Tom is hurt.   \n",
       "1017                       Stop screaming.   \n",
       "1502                     Listen carefully.   \n",
       "3912  Could you speak more slowly, please?   \n",
       "371                            Definitely!   \n",
       "1038                       Tom is wounded.   \n",
       "70                                Goodbye!   \n",
       "1902                   Which cap is yours?   \n",
       "1313                      Stop right here.   \n",
       "387                            Follow Tom.   \n",
       "1163                      I have returned.   \n",
       "\n",
       "                                        Predicted  \n",
       "1210                       i have learning french  \n",
       "1200                              i'm your friend  \n",
       "2247                        don't delude yourself  \n",
       "1257                              is he a teacher  \n",
       "1075                             are you a doctor  \n",
       "1116                             everybody saw it  \n",
       "1052                                    we're are  \n",
       "3004                   can you keep an eye on tom  \n",
       "2924                   i finally found my my well  \n",
       "621                                 tom's injured  \n",
       "1017                                stop shouting  \n",
       "1502                             listen carefully  \n",
       "3912  could you please speak little little please  \n",
       "371                                     of course  \n",
       "1038                                tom's injured  \n",
       "70                                       call the  \n",
       "1902                           which cap is yours  \n",
       "1313                                       you go  \n",
       "387                                    follow tom  \n",
       "1163                                    i'm right  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[100:120,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the above dataframe model has done quiet well to predict english sequence for input bengali sequence."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Russia_To_English_Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
