{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Description:\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.   \n",
    "\n",
    "The surface is described using a grid like the following:  \n",
    "  \n",
    "    SFFF       (S: starting point, safe)\n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole, fall to your doom)\n",
    "    HFFG       (G: goal, where the frisbee is located)\n",
    "    \n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements of Markov Decision Process (MDP):\n",
    ">**Environment** :  The Grid of size (4,4) is the environment (Frozen Lake).\n",
    "\n",
    ">**Agent**: The Software Thing which interactcs with the environment. Agent's starting position in the environment is state 'S'.\n",
    "\n",
    ">**State**: There are 16 states available. state 'S' is starting point in the environment. state 'F' is frozen surface. state 'H' is the hole and state 'G' is the final state or terminate state.\n",
    "\n",
    ">**Action**: 4 Possible actions which agent can take are - left, right, up and down.\n",
    "\n",
    ">**Reward**: Agent would earn reward of 1 when it reaches the terminate state('G') else for eevry other state agent would get reward 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm.\n",
    "Q-Learning is a model-free Reinforcement learning algorithm uses MDP framework.It involves an agent, set of states **S**, and a set **A** of actions per state. Agent Can perform an action **$a\\in A$** from a specific state **$s\\in S$** and recahes to a new state **$s^{'} \\in S$** and also earns a reward **R**.\n",
    "\n",
    "Parameters used in the algorithm are -\n",
    "\n",
    "> **Discounting Factor ($\\gamma$):** It is used to solve the problem in an environment where states are infinite. Agent's goal in Q-Learning RL is to maximize the return (total rewards throughout all steps in an episode). But when states are infinite total rewards also becomes infinite. To address this a discounting factor is added to each future reward . discounting factor gives less importance to immediate reward over future reward and helps agents to get finite total rewards. $\\gamma$ can be between 0 to 1. We will set the parameter value to 0.99.($\\gamma = 0.99$).\n",
    "\n",
    ">**Learning rate ($\\alpha$):** In Q-Learning algo a Q-Table of state action pairs is maintained to hold the Q-value for each state action pair. Q-value in each state gets updated everytime when agent  take specific action from specific state. alpha is a measure of how quickly to adopt the new Q-value. when alpha is 1 old Q-value gets removed completly. if it is between 0 to 1 old Q-values gets updated slowly with new Q-value.\n",
    "\n",
    ">**Exploration Rate ($\\epsilon$):** This tells agent to explore or exploit. when agent starts initially it should explore the environment means it should learn to take action and understand the environment. when agent plays through the environment more it should exploit (use) the information already available to it.There should be a trade off between exploration and exploitation. epsilon is the tradeoff. it can be between 0 to 1. $\\epsilon == 1$ meand 100% probability that agent would explore the environment meaning it would take random action in the environment.\n",
    "\n",
    ">**Number of Episoded**: When agent starts from initial state and reaches to final state it is called 1 episode. in a game like frozen lake an agent can take multiple episodes meaning agent can reach to its goal state many times through different state action pairs. We will allow our agent to learn by completing 10000 such episodes.\n",
    "\n",
    ">**Number of States**: An agent can reach its goal by transitioning from one state to another. we will restrict our agent to reach its goal by taking maximum 100 steps per episodes.If agent does not reach its goal state within 100 steps then it would earn nothing. \n",
    "\n",
    "\n",
    "Lets write the highlevel pseudo code for the implementation of 'Frozen-Lake' game using Q-Learning RL algorithm.\n",
    "\n",
    "1. create environment object.  \n",
    "2. initialize all parameters to be used.  \n",
    "3. initialize q-table of shape (states_count , action_count).    \n",
    "4. Train agent as follows  \n",
    "\n",
    "\n",
    "    for each episode :\n",
    "        set state to first state.\n",
    "        set reward to 0\n",
    "    \n",
    "        for each step:\n",
    "            find action\n",
    "            perform action\n",
    "\n",
    "            update q-table with new q-value for state action pair\n",
    "            update state to new state\n",
    "            update earned reward\n",
    "\n",
    "            if final_state:\n",
    "                break\n",
    "\n",
    "        update epsilon\n",
    "    store reward per episode\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 'Frozen Lake' environment built by OpenAI-gym\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Possible Actions  4\n",
      "Count of Possible States  16\n"
     ]
    }
   ],
   "source": [
    "print('Count of Possible Actions ' , env.action_space.n)\n",
    "print('Count of Possible States ', env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-Table: \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Createing Q-Table and Initializing\n",
    "\n",
    "acion_space_size_ = env.action_space.n\n",
    "state_space_size_ = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size_, acion_space_size_)) \n",
    "print('Initial Q-Table: \\n', q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Q-Learning Parameters\n",
    "gamma_ = 0.99\n",
    "alpha_ = 0.1\n",
    "\n",
    "epsilon_ = 1\n",
    "max_epsilon_ = 1\n",
    "min_epsilon_ = 0.01\n",
    "epsilon_decay_ = 0.001\n",
    "\n",
    "episode_len_ = 10000\n",
    "max_steps_ = 100\n",
    "\n",
    "rewards_per_episode_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-Learning Algorithm - Training\n",
    "\n",
    "for ep in range(episode_len_):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    curr_reward = 0\n",
    "    \n",
    "    for stp in range(max_steps_):\n",
    "        \n",
    "        #Exploration-Exploitation Trade Off\n",
    "        exploration_threshold = random.uniform(0,1)\n",
    "        if exploration_threshold > epsilon_:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        #Performing Action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #Update the Q-Table\n",
    "        q_table[state, action] = (1 - alpha_)*q_table[state, action] + \\\n",
    "                                alpha_ * ( reward + gamma_ * np.max(q_table[new_state, :]) )\n",
    "        \n",
    "        #Updating Current Reward\n",
    "        curr_reward += reward\n",
    "        #print('expl_th ',exploration_threshold)\n",
    "        #print('state ',state)\n",
    "        #print('action ',action)\n",
    "        #print('reward ', curr_reward)\n",
    "        # Updating State to new state\n",
    "        state = new_state\n",
    "        \n",
    "        # Break if agent reached terminate state\n",
    "        if done == True:\n",
    "            break\n",
    "    \n",
    "    # Decaying Exploration Rate\n",
    "    epsilon_ = min_epsilon_ + (max_epsilon_ - min_epsilon_)* np.exp(- epsilon_decay_ * ep)\n",
    "    \n",
    "    # Storing the Reward achieved in an episode\n",
    "    rewards_per_episode_.append(curr_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our agent is completed in above code. Now lets see how agent performed. We will look at the average rewards per thousands episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 : 0.034\n",
      "2000 : 0.197\n",
      "3000 : 0.41\n",
      "4000 : 0.545\n",
      "5000 : 0.62\n",
      "6000 : 0.631\n",
      "7000 : 0.675\n",
      "8000 : 0.671\n",
      "9000 : 0.667\n",
      "10000 : 0.679\n"
     ]
    }
   ],
   "source": [
    "# Check Average reward per thousand episode\n",
    "rewards_per_thousand_episode = np.split(np.array(rewards_per_episode_),(episode_len_/1000))\n",
    "count = 1000\n",
    "for rewards in rewards_per_thousand_episode:\n",
    "    print(count ,':', (sum(rewards)/1000))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that average reward is increased over time. when agent completed first 1000 episodes ,average reward was 3% and when agent completed 10000 episodes , rewards jumps to 67%.\n",
    "\n",
    "Our agent played 10,000 episodes. At each time step within an episode, the agent received a reward of 1 if it reached the frisbee, otherwise, it received a reward of 0. If the agent did indeed reach the frisbee, then the episode finished at that time-step. So, that means for each episode, the total reward received by the agent for the entire episode is either 1 or 0. So, for the first thousand episodes, we can interpret this score as meaning that 3% of the time, the agent received a reward of 1 and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning 67% of the time.  \n",
    "\n",
    "Lets print the final updated Q-Table for state action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Q-Table after 10000 Episodes \n",
      " [[0.54945633 0.52607016 0.50171397 0.52952858]\n",
      " [0.3685751  0.31978314 0.3551354  0.49970314]\n",
      " [0.41680771 0.39045468 0.41030492 0.45702724]\n",
      " [0.24228146 0.28574767 0.27535768 0.44670587]\n",
      " [0.57520308 0.43207537 0.3774571  0.35430742]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2977282  0.14329652 0.15935138 0.08978597]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38025314 0.37792853 0.49447403 0.65553935]\n",
      " [0.51896736 0.6995394  0.45060328 0.29917972]\n",
      " [0.68305296 0.41437302 0.25256196 0.38059724]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41199595 0.60804558 0.76155089 0.47475399]\n",
      " [0.75630301 0.88079808 0.70657757 0.72878231]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Updated Q-Table after 10000 Episodes \\n', q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below lines of code is just to visualise the game ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "# Testing Our Agent - Visualising\n",
    "for episode in range(20):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print('**********EPISODE****** ', episode+1)\n",
    "    \n",
    "    for step in range(max_steps_):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state , reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
